---
title: "Tools"
---
```{r, child="_setup.Rmd"}
```
This is a very abbreviated list of a few select tools and resources that you may find useful in building and analyzing textual data.  

##Text Analysis Tools
For students in writing courses, you may want to conduct some analysis of the texts you are using for your projects.  To do so, there are several ready-made tools that can help you with the analysis.  
  One popular way to analyze texts is to use a concordancer, that will allow you to search for Key Words in Context, see word lists by frequency, and see collocations.  A useful (and free) concordancer is Antconc, created by Lawrence Anthony. Anthony’s [website](https://www.laurenceanthony.net/) offers lots of text analysis software tools like [AntConc](https://www.laurenceanthony.net/software/antconc/) and [AntWordProfiler](https://www.laurenceanthony.net/software/antwordprofiler/), along with thorough documentation and help files. 

* If you are more interested in looking at vocabulary, especially how vocabulary in a text compares with broader trends in vocabulary use, a useful tool is "Lextutor.""  The Compleat [Lexical Tutor](http://lextutor.ca) analyzes the vocabulary used in texts based on several well-documented measures.

* If you want to analyze cohesion or coherence in textual documents, you may want to look into [Coh-Metrix](http://cohmetrix.com/).  

##Sources for language data
* [The Internet Archive](https://archive.org/) is a valuable repository for classic literature, pop literature, music, and other media that are out of copyright. If you are interested in collecting a large dataset from the archive, read this [helpful tutorial](https://programminghistorian.org/en/lessons/data-mining-the-internet-archive) from the *Programming Historian*.
* [TED talks](https://www.ted.com/talks) – Video and transcripts of high-interest presentations by recognized experts. 
* [New South Voices](https://nsv.uncc.edu/) - Interviews from people in the Southeast (especially in the Charlotte area)

##Web-based Corpora

If you want to analyze texts or language use already out there in the universe, there are several web-based tools that allows you to analyze existing data sets.  Here are a few useful 

* [Corpus of Contemporary American English](http://corpus.byu.edu/coca/)
* [The Spoken British National Corpus](https://t.co/RpKT3SkBJG)
* [Corpus of Historical American English](http://corpus.byu.edu/coha/) – if you want to see how language changes over time.
* [SACODEYL](https://www.um.es/sacodeyl/) – A corpus of different European languages with transcripts and video.  The website seems to be abandoned, but most of the materials still work.  
* [MICASE](https://quod.lib.umich.edu/cgi/c/corpus/corpus?c=micase;page=simple) – Michigan Corpus of Academic Spoken English
* [MICUSP](http://micase.elicorpora.info) – Michigan Corpus of Upper-Level Student Papers
* [Trump Twitter Archive](http://trumptwitterarchive.com/archive) – Searchable database of all of Trump's Tweets (and several other related Twitter accounts)
* [Wikipedia Corpus](https://corpus.byu.edu/wiki/) – quickly create your own corpus of Wikipedia articles on related topics.

##Cleaning and managing data

* OpenRefine
* [AntFileConverter](http://www.laurenceanthony.net/software/antfileconverter/)

